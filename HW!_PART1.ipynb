{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "262ee4f1cc793d5cc0302d24cbc64461",
     "grade": false,
     "grade_id": "cell-aa820d6aaf4304db",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "version = \"v1.6.092820\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "288260708ed53d23fbee2b1333a77e8a",
     "grade": false,
     "grade_id": "cell-24e63ee011a83003",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "# Assignment 1 Part 1: N-gram Language Models (40 pts)\n",
    "\n",
    "In this assignment, we're going to train an n-gram language model that is able to \"imitate\" William Shakespeare's writing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d42c6f82b52e85920cee3cca49b7239a",
     "grade": false,
     "grade_id": "cell-0153dc3ed86e1f61",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Configure nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk_data_path = \"assets/nltk_data\"\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d11c9af18458f110af7afffe909f9b86",
     "grade": false,
     "grade_id": "cell-d63bebc6fef0f0d1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 1: Load the dataset (10 pts)\n",
    "\n",
    "As the first step towards imitating Shakespeare's writing, you will create a function called `load_data` that loads his original *Sonnets* from `assets/gutenberg/THE_SONNETS.txt`. This function should accomplish the following: \n",
    "\n",
    "* **Extract sentences from the data file.** Of course, depending on the nature of the task at hand, what constitutes a *sentence* can vary. In the context of this assignment, we will define a sentence as just a line of a sonnet, regardless of the punctuation at end. In addition, we will ignore the boundaries of the sonnets --- that is, we are not dealing with $154$ individual *sonnets* but rather $154 \\times 14 = 2156$ *sentences* (actually only $2155$ sentences, as *Sonnet 99* has [15 lines](https://www.google.com/search?ei=po4hX9jzN4rKtQaL7K-wDg&q=why+is+sonnet+99+15+lines&oq=why+is+sonnet+99+15+lines&gs_lcp=CgZwc3ktYWIQAzoECAAQR1ChGlihGmCqHGgAcAJ4AIABXIgBXJIBATGYAQCgAQGqAQdnd3Mtd2l6wAEB&sclient=psy-ab&ved=0ahUKEwjY3snX3PLqAhUKZc0KHQv2C-YQ4dUDCAw&uact=5) but *Sonnet 126* has only [12](https://www.google.com/search?ei=d4whX6_uH9a4tQbUiISoDA&q=why+is+sonnet+126+only+12+lines&oq=o+thou+my+lovely+boy+who+in+thy+power&gs_lcp=CgZwc3ktYWIQARgBMgQIABBHMgQIABBHMgQIABBHMgQIABBHMgQIABBHMgQIABBHMgQIABBHMgQIABBHUABYAGCWEWgAcAJ4AIABAIgBAJIBAJgBAKoBB2d3cy13aXrAAQE&sclient=psy-ab)). We encourage you to explore alternative definitions of a sentence on your own; for example, an entire sonnet could be modelled as a sentence. Finally, make sure that the newline character `\\n` at the end of each line is dropped. \n",
    "\n",
    "\n",
    "* **Tokenise each extracted sentence.** While it's ambiguous what a sentence is, what constitutes a \"*word*\" is even more task-dependent. Do punctuations count as \"words\"? Are two \"words\" with the same spelling but different casing considered identical? Since what a text file contains is nothing more than a squence of characters, there are many possible ways of grouping these characters to form \"words\" that are subsequently taken as input by a program. To distinguish what's actually taken as input by a program from a linguistic word, we call the former a *token*. The process of producing a list of tokens out of a sentence is then called *tokenisation*. At this step, you will first lower-case each sentence extracted from the previous step entirely and then tokenise each lower-cased sentence. You may use any tokeniser of your choice, such as `word_tokenize` from the `nltk` library. The grading of the assignment doesn't depend on your choice of the tokeniser. \n",
    "\n",
    "**This function should return a `list` of length 2155, where each element is a `list` of `str` representing the tokens of each sentence as produced by the tokeniser of your choice. An example output would be:**\n",
    "\n",
    "```\n",
    "[['from', 'fairest', 'creatures', 'we', 'desire', 'increase', ','],\n",
    " ['that', 'thereby', 'beauty', '’', 's', 'rose', 'might', 'never', 'die', ','],\n",
    " ...\n",
    " ['came', 'there', 'for', 'cure', 'and', 'this', 'by', 'that', 'i', 'prove', ','], \n",
    " ['love', '’', 's', 'fire', 'heats', 'water', ',', 'water', 'cools', 'not', 'love', '.']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4d4ea2201f33d2455eb83f6c7c68e483",
     "grade": false,
     "grade_id": "cell-876ee5c9fdc4f347",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load text data from a file and produce a list of token lists\n",
    "    \"\"\"\n",
    "    is_integer = lambda s: s.isdigit() or (s[0] == '-' and s[1:].isdigit())\n",
    "    with open('assets/gutenberg/THE_SONNETS.txt', 'r') as f:\n",
    "        sentences_no_newline = [line.strip() for line in f]\n",
    "        sentences_no_empty_lines = list(filter(None, sentences_no_newline))\n",
    "        sentences_no_integer = [item for item in sentences_no_empty_lines if not item.isdigit()]\n",
    "        sentences_tokenized = [word_tokenize(i) for i in sentences_no_integer]\n",
    "        sentences = [[j.lower() for j in i] for i in sentences_tokenized]\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3e4184bbeb9f594851ab27528c1c385e",
     "grade": true,
     "grade_id": "cell-436f8cfe2405ffe8",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test\n",
    "\n",
    "stu_ans = load_data()\n",
    "\n",
    "assert isinstance(stu_ans, list), \"Q1: Your function should return a list. \"\n",
    "assert len(stu_ans) == 2155, \"Q1: There should be 2155 sentences. \"\n",
    "\n",
    "for index, tokens in enumerate(stu_ans):\n",
    "    assert isinstance(tokens, list), f\"Q1: The element at index {index} of your answer list should be a list. \"\n",
    "    for token in tokens:\n",
    "        if token.isalpha():\n",
    "            assert token.islower(), f'Q1: Token \"{token}\" in the sentence at index {index} is not lower-cased. '\n",
    "        \n",
    "        assert token != \"\\n\", f'Q1: You should drop the \"\\\\n\" character in the sentence at index {index}. '\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b485ef9c888bceed2e2e50147e1e7955",
     "grade": false,
     "grade_id": "cell-902e515e7633ecbd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 2: Build vocabulary (15 pts)\n",
    "\n",
    "Next, we need a \"vocabulary\" that contains all the unique tokens. Moreover, as mentioned in the lecture, we often pad a sentence with `<s>` and `</s>` to indicate its start and end when working with n-gram language models; therefore, these two special tokens should also be included in our vocabulary. Complete the function below to build a vocabulary. The order in which the tokens are stored doesn't matter. \n",
    "\n",
    "**This function should return a `list` of unique tokens, including `<s>` and `</s>`. An example output would be:**\n",
    "\n",
    "```\n",
    "['refuse',\n",
    " 'enjoyed',\n",
    " ...\n",
    " '<s>', \n",
    " '</s>']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "19516439848342b7eedc50ef78c20fc7",
     "grade": false,
     "grade_id": "cell-be2a75b95ae4c066",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Take a list of sentences and return a vocab\n",
    "    \"\"\"\n",
    "    \n",
    "    list_of_sentences = sentences\n",
    "    \n",
    "    vocab_without_specialtoken = list(set(chain(*list_of_sentences)))\n",
    "    vocab_without_specialtoken.extend(['<s>', '</s>' ])\n",
    "    vocab = vocab_without_specialtoken                                   \n",
    "    \n",
    "   \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ee92ac4607ae41ab4060b198642004c1",
     "grade": true,
     "grade_id": "cell-bd592f77659516e2",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_sents = load_data()\n",
    "stu_vocab = build_vocab(stu_sents)\n",
    "\n",
    "assert isinstance(stu_vocab, list), \"Q2: Your function should return a list. \"\n",
    "assert stu_vocab, \"Q2: Your vocab is empty. \"\n",
    "assert \"<s>\" in stu_vocab, \"Q2: Remember to include special token <s>. \"\n",
    "assert \"</s>\" in stu_vocab, \"Q2: Remember to include special token </s>. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_sents, stu_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d7c6f4ee7affc1ab88ddd68db23652d7",
     "grade": false,
     "grade_id": "cell-4b22a356806c732d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 3: Generate all $n$-grams (15 pts)\n",
    "\n",
    "Now let's write a function to generate all $n$-grams for each sentence. This can be accomplished in two steps:\n",
    "* **Pad each sentence with `<s>` and `</s>` for $n \\geq 2$**. You need $n - 1$ paddings on both ends of a sentence, so that there are two $n$-grams that model the first and the last token, respectively. You may implement the padding function yourself or use the `pad_both_ends` function from the `nltk` library. \n",
    "\n",
    "\n",
    "* **Generate $n$-grams on the padded sentences**. Check out the `ngrams` function from `nltk`. For a sentence with $\\ell$ tokens excluding paddings, the maximum number of possible $n$-grams generated from its padded version should be $\\ell + n - 1$. Think about why. \n",
    "\n",
    "**Complete the function below to return a `list`, where each element of the `list` is a either a list or a \"generator object\" produced by the `ngrams` function, representing a sequence of all $n$-grams generated from each appropriately padded sentence. If the argument `n=2`, the autograder would accept either of the example outputs below:**\n",
    "\n",
    "```\n",
    "[<generator object ngrams at 0x7f77ed778b10>,\n",
    " <generator object ngrams at 0x7f77e7d934f8>,\n",
    " ...\n",
    " <generator object ngrams at 0x7f77e7d751b0>,\n",
    " <generator object ngrams at 0x7f77e7d75228>]\n",
    "```\n",
    "\n",
    "**OR**\n",
    "\n",
    "```\n",
    "[\n",
    "    [('<s>', 'from'), ('from', 'fairest'), ('fairest', 'creatures'), ('creatures', 'we'), ('we', 'desire'),\n",
    "     ('desire', 'increase'), ('increase', ','), (',', '</s>')], \n",
    "    \n",
    "    [('<s>', 'that'), ('that', 'thereby'), ('thereby', 'beauty'), ('beauty', '’'), ('’', 's'), ('s', 'rose'), \n",
    "     ('rose', 'might'), ('might', 'never'), ('never', 'die'), ('die', ','), (',', '</s>')],\n",
    "     \n",
    "    ...\n",
    "    \n",
    "    [('<s>', 'came'), ('came', 'there'), ('there', 'for'), ('for', 'cure'), ('cure', 'and'), ('and', 'this'), \n",
    "     ('this', 'by'), ('by', 'that'), ('that', 'i'), ('i', 'prove'), ('prove', ','), (',', '</s>')], \n",
    "    \n",
    "    [('<s>', 'love'), ('love', '’'), ('’', 's'), ('s', 'fire'), ('fire', 'heats'), ('heats', 'water'), \n",
    "     ('water', ','), (',', 'water'), ('water', 'cools'), ('cools', 'not'), ('not', 'love'), ('love', '.'), \n",
    "     ('.', '</s>')]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c82c4eeada18079f4459f3e5d7242912",
     "grade": false,
     "grade_id": "cell-79211162e032488b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "def build_ngrams(n, sentences):\n",
    "    \"\"\"\n",
    "    Take a list of unpadded sentences and create all n-grams as specified by the argument \"n\" for each sentence\n",
    "    \"\"\"\n",
    "    updated_sentences = []\n",
    "    all_ngrams = []\n",
    "    \n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = list(pad_both_ends(sentence,  n=n))\n",
    "        updated_sentences.append(sentence)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    for sentence in updated_sentences:\n",
    "        sentence = list(ngrams(sentence, n))\n",
    "        all_ngrams.append(sentence)\n",
    "        \n",
    "    \n",
    "    \n",
    "    return list(all_ngrams)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cb074e581ca1460bd58b6f6493138328",
     "grade": true,
     "grade_id": "cell-66bf33f44f8b83bf",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_n = 4\n",
    "stu_sents = load_data()\n",
    "stu_ngrams = build_ngrams(stu_n, stu_sents)\n",
    "\n",
    "assert isinstance(stu_ngrams, list), \"Q3: Your function should return a list. \"\n",
    "assert stu_ngrams, \"Q3: Your ngrams list is empty. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "\n",
    "del stu_n, stu_sents, stu_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ad086d5cfb862a0e623e123e8fe7e500",
     "grade": false,
     "grade_id": "cell-caf5a40a85230775",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now that we have completed all the preparation work for imitating William Shakespeare's writing, it's time to take a break. We will resume in Assignment 1 Part 2 to finish training an $n$-gram language model. See you there!"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_data_mining_ii_v1_assignment1_part1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
