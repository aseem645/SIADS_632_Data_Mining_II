{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "262ee4f1cc793d5cc0302d24cbc64461",
     "grade": false,
     "grade_id": "cell-aa820d6aaf4304db",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "version = \"v1.6.092820\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "abe9e48010928487f05fb692a8fc93d1",
     "grade": false,
     "grade_id": "cell-24e63ee011a83003",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "# Assignment 1 Part 2: N-gram Language Models (Cont.) (30 pts)\n",
    "\n",
    "In this assignment, we're going to train an n-gram language model that is able to \"imitate\" William Shakespeare's writing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d42c6f82b52e85920cee3cca49b7239a",
     "grade": false,
     "grade_id": "cell-0153dc3ed86e1f61",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Configure nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk_data_path = \"assets/nltk_data\"\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "73e5265758b71680e006b1db39c741bf",
     "grade": false,
     "grade_id": "cell-125dc5645c8de34a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Copy and paste the functions you wrote in Part 1 here and import any libraries necessary\n",
    "# We have tried a more elegant solution by using\n",
    "# from ipynb.fs.defs.assignment1_part1 import load_data, build_vocab, build_ngrams\n",
    "# but it doesn't work with the autograder...\n",
    "\n",
    "from itertools import chain\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "import pandas as pd\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load text data from a file and produce a list of token lists\n",
    "    \"\"\"\n",
    "    is_integer = lambda s: s.isdigit() or (s[0] == '-' and s[1:].isdigit())\n",
    "    with open('assets/gutenberg/THE_SONNETS.txt', 'r') as f:\n",
    "        sentences_no_newline = [line.strip() for line in f]\n",
    "        sentences_no_empty_lines = list(filter(None, sentences_no_newline))\n",
    "        sentences_no_integer = [item for item in sentences_no_empty_lines if not item.isdigit()]\n",
    "        sentences_tokenized = [word_tokenize(i) for i in sentences_no_integer]\n",
    "        sentences = [[j.lower() for j in i] for i in sentences_tokenized]\n",
    "    \n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Take a list of sentences and return a vocab\n",
    "    \"\"\"\n",
    "    \n",
    "    list_of_sentences = sentences\n",
    "    \n",
    "    vocab_without_specialtoken = list(set(chain(*list_of_sentences)))\n",
    "    vocab_without_specialtoken.extend(['<s>', '</s>' ])\n",
    "    vocab = vocab_without_specialtoken                                   \n",
    "    \n",
    "   \n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def build_ngrams(n, sentences):\n",
    "    \"\"\"\n",
    "    Take a list of unpadded sentences and create all n-grams as specified by the argument \"n\" for each sentence\n",
    "    \"\"\"\n",
    "    updated_sentences = []\n",
    "    all_ngrams = []\n",
    "    \n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = list(pad_both_ends(sentence,  n=n))\n",
    "        updated_sentences.append(sentence)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    for sentence in updated_sentences:\n",
    "        sentence = list(ngrams(sentence, n))\n",
    "        all_ngrams.append(sentence)\n",
    "        \n",
    "    \n",
    "    \n",
    "    return list(all_ngrams)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d806cda7a05f3e55d7bc9ed2ad72427f",
     "grade": false,
     "grade_id": "cell-e4ca1cd2de4ef7da",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 4: Guess the next token (20 pts)\n",
    "\n",
    "Let's first warm ourselves up by answering the following question as a review on $n$-grams:\n",
    "\n",
    "Assume we are now working with bi-grams. What is the most likely token that comes after the sequence `<s> <s> <s>`, and how likely? Remember that a bi-gram language model is essentially a first-order Markov Chain. So, what determines the next state in a first-order Markov Chain? \n",
    "\n",
    "**Complete the function below to return a `tuple`, where `tuple[0]` is a `str` representing the mostly likely token and `tuple[1]` is a `float` representing its (conditional) probability of being the next token.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7d19295ea8f314fb6b49f963689e9609",
     "grade": false,
     "grade_id": "cell-f0590ac9b51f337e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def bigram_next_token(start_tokens=(\"<s>\", ) * 3):\n",
    "    \"\"\"\n",
    "    Take some starting tokens and produce the most likely token that follows under a bi-gram model\n",
    "    \"\"\"\n",
    "    \n",
    "    zero_sublist = []\n",
    "    one_sublist = []\n",
    "    both_sublist = []\n",
    "    \n",
    "    ngram_length = 2\n",
    "    sentences = load_data()\n",
    "    bigrams = build_ngrams(ngram_length, sentences)\n",
    "    \n",
    "    for sentence in bigrams:\n",
    "        for item in sentence:\n",
    "            zero_sublist.append(item[0])\n",
    "            \n",
    "    for sentence in bigrams:\n",
    "        for item in sentence:\n",
    "            one_sublist.append(item[1])\n",
    "            \n",
    "    for sentence in bigrams:\n",
    "        for item in sentence:\n",
    "            both_sublist.append(item)\n",
    "            \n",
    "    bigrams_df = pd.DataFrame(\n",
    "    {'0': zero_sublist,\n",
    "     '1': one_sublist,\n",
    "     '2grams': both_sublist\n",
    "    })\n",
    "    \n",
    "    bigrams_df['2grams'] = bigrams_df['2grams'].astype(str)\n",
    "    bigrams_df['2grams'] = bigrams_df['2grams'].str.replace('(', '')\n",
    "    bigrams_df['2grams'] = bigrams_df['2grams'].str.replace(')', '')\n",
    "    bigrams_df['2grams'] = bigrams_df['2grams'].str.replace(\"'\", '')\n",
    "    \n",
    "    filters = bigrams_df[\"2grams\"].str.startswith('<s>') \n",
    "    bigrams_df = bigrams_df[filters]\n",
    "    bigrams_df['freq'] = bigrams_df.groupby('1')['1'].transform('count')\n",
    "    bigrams_df['probability'] = bigrams_df['freq']/2155\n",
    "    max_prob = bigrams_df[bigrams_df.probability == bigrams_df.probability.max()]\n",
    "    max_prob['probability'] = max_prob['probability'].astype(float)\n",
    "    max_prob['1'] = max_prob['1'].astype(str)\n",
    "    max_prob_row = max_prob.iloc[0, :].values.tolist()\n",
    "    the_token = max_prob_row[1]\n",
    "    probability = max_prob_row[4]\n",
    "            \n",
    "\n",
    "    \n",
    "    next_token, prob = the_token, probability\n",
    "    \n",
    "\n",
    "    \n",
    "    return next_token, prob\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "da8b5be8d6a7ec8ba658634873e4edf4",
     "grade": true,
     "grade_id": "cell-6a94a4513c67c4d5",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_ans = bigram_next_token(start_tokens=(\"<s>\", ) * 3)\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Q4: Your function should return a tuple. \"\n",
    "assert len(stu_ans) == 2, \"Q4: Your tuple should have two elements. \"\n",
    "assert isinstance(stu_ans[0], str), \"Q4: tuple[0] should be a str. \"\n",
    "assert isinstance(stu_ans[1], float), \"Q4: tuple[1] should be a float. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fbc1bde605ee9c155fc7a18fea7244cc",
     "grade": false,
     "grade_id": "cell-4b22a356806c732d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 5: Train an $n$-gram language model (10 pts)\n",
    "\n",
    "Now we are well positioned to start training an $n$-gram language model. We can fit a language model using the `MLE` class from `nltk.lm`. It requires two inputs: a list of all $n$-grams for each sentence and a vocabulary, both of which you have already written a function to build. Now it's time to put them together to work. \n",
    "\n",
    "**Complete the function below to return a trained $n$-gram language model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9c62ee69084d283bba34ee6a78a90907",
     "grade": false,
     "grade_id": "cell-6136d38621377eeb",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "\n",
    "def train_ngram_lm(n):\n",
    "    \"\"\"\n",
    "    Train a n-gram language model as specified by the argument \"n\"\n",
    "    \"\"\"\n",
    "    \n",
    "    ngrams = n\n",
    "    sentences = load_data()\n",
    "    list_of_ngrams = build_ngrams(ngrams, sentences)\n",
    "    vocabulary = build_vocab(sentences)\n",
    "    \n",
    "    \n",
    "    lm = MLE(n)\n",
    "    lm.fit(list_of_ngrams, vocabulary )\n",
    "\n",
    "    \n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b177bd542dd0d0e1773c3e9fa7cec092",
     "grade": true,
     "grade_id": "cell-9c44953c467910db",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_n = 4\n",
    "stu_lm = train_ngram_lm(stu_n)\n",
    "stu_vocab = build_vocab(load_data())\n",
    "\n",
    "assert hasattr(stu_lm, \"vocab\") and len(stu_lm.vocab) == len(stu_vocab) + 1, \"Q3b: Your language model wasn't trained properly. \"\n",
    "\n",
    "del stu_n, stu_lm, stu_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a096876626b3ba060807018d89d3efd2",
     "grade": false,
     "grade_id": "cell-2fc6d275a5179f63",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "FINALLY, are you ready to compose sonnets like the real Shakespeare?! We provide some starter code below, but absolutely feel free to modify any parts of it on your own. It'd be interesting to see how the \"authenticity\" of the sonnets is related to the parameter $n$. Do the sonnets feel more Shakespeare when you increase $n$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thy self away , art present still with\n",
      "is lust in action , and till action\n",
      "i will be true despite thy scythe and\n",
      "dulling my lines , and doing me disgrace\n",
      "but like a sad slave stay and think\n",
      "save breed to brave him , when he\n",
      "such is my love , to thee i\n",
      "thine eyes , that taught the dumb on\n",
      "points on me graciously with fair aspect ,\n",
      "against confounding age ’ s cruel knife ,\n",
      "against the wrackful siege of batt ’ ring\n",
      "whose worth ’ s unknown , although his\n",
      "ay , fill it full with wills ,\n",
      "nay if thou lour ’ st on me\n"
     ]
    }
   ],
   "source": [
    "# Every time it runs, depending on its mood, a different sonnet is written. \n",
    "n = 8\n",
    "num_lines = 14\n",
    "num_words_per_line = 8\n",
    "text_seed = [\"<s>\"] * (n - 1)\n",
    "\n",
    "lm = train_ngram_lm(n)\n",
    "\n",
    "sonnet = []\n",
    "while len(sonnet) < num_lines:\n",
    "    while True:  # keep generating a line until success\n",
    "        try:\n",
    "            line = lm.generate(num_words_per_line, text_seed=text_seed)\n",
    "        except ValueError:  # the generation is not always successful. need to capture exceptions\n",
    "            continue\n",
    "        else:\n",
    "            line = [x for x in line if x not in [\"<s>\", \"</s>\"]]\n",
    "            sonnet.append(\" \".join(line))\n",
    "            break\n",
    "\n",
    "# pretty-print your sonnet\n",
    "print(\"\\n\".join(sonnet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_data_mining_ii_v1_assignment1_part2"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
